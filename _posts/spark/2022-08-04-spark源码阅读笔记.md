---
layout: post
title:  "spark 源码阅读记录"
date:   2022-08-04 07:25:10 +0800
categories: note
tags: spark scala
---


### RelationalGroupedDataset

`A::Nil` 使用 将单字符变成 集合 比 List(A) 效率高一些

`java.util.locale` 国际化的工具类



#### 配置

```scala
spark.sql.retainGroupColumns // groupBy 是否包含group col
```

### Column

```code
def this(name: String) = this(name match {/*code*/}) // 构造器使用模式匹配
```

### org.apache.spark.sql.catalyst.analysis

```scala
class UnresolvedAttribute 
// backtick  反引号
org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.parseAttributeName 
// 对做了合法性检查 后将各条目分开
```

### org.apache.spark.sql.catalyst.expressions



### org.apache.spark.sql.execution.metrix

`SQLMetrix` 监控物理执行

`SQLExecution`

### org.apache.spark.sql.Dataset[T]



```scala
// 执行action 顺便监控执行时间 等信息
def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) [U]
```

### spark RPC

Remote Procedure Call (通信)

spark 2 之前的版本使用akka 做rpc 。 spark2使用自己的`spark-network-common` 底层通信做Rpc

```scala
RpcEndPoint=>Actor
RpcEndPointRef=>ActorRef
RpcEnv=>ActorSystem
```



### scala.match

```scala
val X=1
val x=2

val m=(s:Int)=>{
    s match {
        case X=>"大写"
        case `x`=>"小写"
        case oter @ (5|6|7)=>"@"       
        case x=>"普通"
    }
}
Seq(1,2,3,5).foreach(x=>println(m(x)))// 大写 小写 普通 @

val is8 = "^8$".r
println {
    "8" match {
        case is8() => "unapplySeq"
        case x => "普通"
    }

}
```



### 函数和方法

1. 方法是类的一部分 ，而函数是一个对象可以复制给一个变量
2. 传值调用（call-by-value ）和传名调用(call-by-name)

```
val time=()=>System.nanoTime()
def call_by_value(t:Long){print(t)}
def call_by_name(t:()=>Long){print(t())}
```

3. 部分应用函数

   ```
   val sum=(a:Int,b:Int,c:Int)=>a+b+c // 普通函数
   val sumPart=sum(_:Int,2,_:Iint)
   val sumCurry=(a:Int)=>(b:Int)=>(c:Int)=>a+b+c
   ```

4. scala 中的函数代码块编译后会变成构造器的一部分;

### bio 和 nio

1. nio 规避了并发量太大时线程太多的